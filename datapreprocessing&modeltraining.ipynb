{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a9bcc42-355b-48f2-8393-6f8dc469e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3b79baa-ef49-4dd0-8637-6ec3a65d3f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47543396-ea9f-49ac-992e-4a5d70d64501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432081df-646f-4f8f-a0b6-40b1518c144b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74a77312-e2ad-47fa-a6ac-a601de9d4c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b46fd58b-675a-403b-bfb5-081380f3d5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "011f6f2a-cc3d-4832-aa11-f6057409eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3baa461-fb2a-42f5-b2a5-870116ddbcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove punctuation and lowercase\n",
    "    import re\n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    # Tokenize and remove stop words\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fe9d6d8-065b-4efa-b2b0-675aa966ebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the data\n",
    "X = tfidf.fit_transform(df['cleaned_review']).toarray()\n",
    "\n",
    "# Encode the labels\n",
    "y = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64cfd87f-f7a5-4175-8104-46057caf77da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80c892aa-5a6d-4d01-a16e-5587759b8cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8873\n",
      "Precision: 0.8780440664862775\n",
      "Recall: 0.901567771383211\n",
      "F1 Score: 0.8896504455106237\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab5009",
   "metadata": {},
   "source": [
    "## Feature Engineering for Naive Bayes Sentiment Analysis\n",
    "#### Objectives\n",
    "- Implement text vectorization for Naive Bayes\n",
    "- Calculate and interpret cosine similarity\n",
    "- Prepare features for model training\n",
    "- Document findings for technical report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b0468d-a25e-4a9b-9d0e-7bf575c1a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up All necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For sparse matrix operations\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d936ee77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Checking preprocessed data...\n",
      "DataFrame shape: (50000, 3)\n",
      "Columns: ['review', 'sentiment', 'cleaned_review']\n",
      "First cleaned review: one reviewer mentioned watching 1 oz episode youll hooked right exactly happened mebr br first thing...\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "positive    25000\n",
      "negative    25000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify we have the preprocessed data\n",
    "print(\"üìä Checking preprocessed data...\")\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"First cleaned review: {df['cleaned_review'].iloc[0][:100]}...\")\n",
    "\n",
    "# Check sentiment distribution\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64228f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Splitting data into 80% train, 20% test...\n",
      "‚úÖ Split complete!\n",
      "Training samples: 40000\n",
      "Testing samples: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"üéØ Splitting data into 80% train, 20% test...\")\n",
    "\n",
    "# Split features and target\n",
    "X = df['cleaned_review']\n",
    "y = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Perform stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Split complete!\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "\n",
    "# Save the splits\n",
    "train_df = pd.DataFrame({'text': X_train, 'sentiment': y_train})\n",
    "test_df = pd.DataFrame({'text': X_test, 'sentiment': y_test})\n",
    "train_df.to_csv('train_split.csv', index=False)\n",
    "test_df.to_csv('test_split.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be2ea289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Creating Bag-of-Words features...\n",
      "‚úÖ BOW features created\n",
      "   Vocabulary size: 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß Creating Bag-of-Words features...\")\n",
    "\n",
    "bow_vectorizer = CountVectorizer(\n",
    "    max_features=5000,\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1)\n",
    ")\n",
    "\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"‚úÖ BOW features created\")\n",
    "print(f\"   Vocabulary size: {len(bow_vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d32bf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìê Calculating cosine similarity...\n",
      "Found 8 sentiment words in vocabulary\n",
      "‚úÖ Saved results to cosine_similarity_results.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"üìê Calculating cosine similarity...\")\n",
    "\n",
    "bow_vocab = bow_vectorizer.get_feature_names_out()\n",
    "X_train_bow_matrix = X_train_bow\n",
    "\n",
    "# Select sentiment words\n",
    "sentiment_words = ['good', 'bad', 'great', 'terrible', 'excellent', 'awful', 'love', 'hate']\n",
    "available_words = [word for word in sentiment_words if word in bow_vocab]\n",
    "\n",
    "print(f\"Found {len(available_words)} sentiment words in vocabulary\")\n",
    "\n",
    "if len(available_words) >= 2:\n",
    "    # Get indices and vectors\n",
    "    word_indices = [np.where(bow_vocab == word)[0][0] for word in available_words]\n",
    "    word_vectors = X_train_bow_matrix[:, word_indices].T.toarray()\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity_matrix = cosine_similarity(word_vectors)\n",
    "    \n",
    "    # Save results\n",
    "    similarity_df = pd.DataFrame(\n",
    "        similarity_matrix,\n",
    "        index=available_words,\n",
    "        columns=available_words\n",
    "    )\n",
    "    similarity_df.to_csv('cosine_similarity_results.csv')\n",
    "    print(f\"‚úÖ Saved results to cosine_similarity_results.csv\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not enough sentiment words found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccf77313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving features...\n",
      "‚úÖ All features saved!\n"
     ]
    }
   ],
   "source": [
    "print(\"üíæ Saving features...\")\n",
    "\n",
    "output_dir = Path('feature_engineering_output')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save BOW features\n",
    "sp.save_npz(output_dir / 'bow_X_train.npz', X_train_bow)\n",
    "sp.save_npz(output_dir / 'bow_X_test.npz', X_test_bow)\n",
    "\n",
    "# Save vectorizer\n",
    "with open(output_dir / 'bow_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(bow_vectorizer, f)\n",
    "\n",
    "# Save labels\n",
    "np.save(output_dir / 'y_train.npy', y_train.values)\n",
    "np.save(output_dir / 'y_test.npy', y_test.values)\n",
    "\n",
    "print(\"‚úÖ All features saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
